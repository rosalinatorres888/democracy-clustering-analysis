{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c56fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f2834cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation Entropy and Complexity Analysis\n",
      "------------------------------------------\n",
      "1. Use sample generated data (for testing)\n",
      "2. Load processed data from CSV file\n",
      "Enter your choice (1/2): 2\n",
      "Enter the path to the CSV file: /Users/rosalinatorres/Downloads/processed_permutation_entropy_complexity.csv\n",
      "Loading data from /Users/rosalinatorres/Downloads/processed_permutation_entropy_complexity.csv...\n",
      "Warning: Loaded data is missing columns: ['subject', 'activity', 'axis', 'embedded_dim', 'embedded_delay', 'signal_length', 'permutation_entropy', 'complexity']\n",
      "Cannot proceed with visualization.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Function to calculate permutation pattern\n",
    "def get_permutation_pattern(time_series, start_idx, embedding_dim, delay):\n",
    "    \"\"\"\n",
    "    Extract the permutation pattern from a time series segment.\n",
    "    \n",
    "    Args:\n",
    "        time_series: Input time series data\n",
    "        start_idx: Starting index\n",
    "        embedding_dim: Embedding dimension\n",
    "        delay: Time delay\n",
    "        \n",
    "    Returns:\n",
    "        Pattern index representing the ordinal pattern\n",
    "    \"\"\"\n",
    "    # Extract values for the pattern\n",
    "    pattern_values = [time_series[start_idx + i * delay] for i in range(embedding_dim)]\n",
    "    \n",
    "    # Get the indices that would sort the pattern\n",
    "    sorted_indices = np.argsort(pattern_values)\n",
    "    \n",
    "    # Convert to a single integer representation (factorial number system)\n",
    "    pattern_index = 0\n",
    "    base = 1\n",
    "    for i in range(embedding_dim - 1, -1, -1):\n",
    "        pattern_index += sorted_indices[i] * base\n",
    "        base *= (i + 1)\n",
    "    \n",
    "    return pattern_index\n",
    "\n",
    "# Function to calculate permutation entropy\n",
    "def calculate_permutation_entropy(time_series, embedding_dim, delay, normalize=True):\n",
    "    \"\"\"\n",
    "    Calculate permutation entropy for a time series.\n",
    "    \n",
    "    Args:\n",
    "        time_series: Input time series data\n",
    "        embedding_dim: Embedding dimension\n",
    "        delay: Time delay\n",
    "        normalize: Whether to normalize the entropy value\n",
    "        \n",
    "    Returns:\n",
    "        Permutation entropy value\n",
    "    \"\"\"\n",
    "    # Ensure time series is NumPy array\n",
    "    time_series = np.array(time_series)\n",
    "    \n",
    "    # Calculate the number of patterns possible\n",
    "    n_patterns = math.factorial(embedding_dim)\n",
    "    \n",
    "    # Array to store pattern counts\n",
    "    pattern_counts = np.zeros(n_patterns)\n",
    "    \n",
    "    # Calculate valid length for patterns\n",
    "    valid_length = len(time_series) - (embedding_dim - 1) * delay\n",
    "    \n",
    "    # Count patterns\n",
    "    for i in range(valid_length):\n",
    "        pattern = get_permutation_pattern(time_series, i, embedding_dim, delay)\n",
    "        pattern_counts[pattern] += 1\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    pattern_probs = pattern_counts / valid_length\n",
    "    \n",
    "    # Remove zero probabilities for entropy calculation\n",
    "    pattern_probs = pattern_probs[pattern_probs > 0]\n",
    "    \n",
    "    # Calculate entropy\n",
    "    pe = entropy(pattern_probs, base=math.e)\n",
    "    \n",
    "    # Normalize if requested\n",
    "    if normalize and pe != 0:\n",
    "        pe = pe / np.log(n_patterns)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# Function to calculate complexity based on permutation entropy\n",
    "def calculate_complexity(time_series, embedding_dim, delay):\n",
    "    \"\"\"\n",
    "    Calculate statistical complexity based on permutation entropy.\n",
    "    \n",
    "    Args:\n",
    "        time_series: Input time series data\n",
    "        embedding_dim: Embedding dimension\n",
    "        delay: Time delay\n",
    "        \n",
    "    Returns:\n",
    "        Complexity value\n",
    "    \"\"\"\n",
    "    # Calculate permutation entropy (normalized)\n",
    "    pe = calculate_permutation_entropy(time_series, embedding_dim, delay, normalize=True)\n",
    "    \n",
    "    # Calculate disequilibrium (using Jensen-Shannon divergence approximation)\n",
    "    n_patterns = math.factorial(embedding_dim)\n",
    "    \n",
    "    # Uniform distribution (maximum entropy)\n",
    "    uniform_prob = 1.0 / n_patterns\n",
    "    \n",
    "    # Calculate pattern probabilities\n",
    "    pattern_counts = np.zeros(n_patterns)\n",
    "    valid_length = len(time_series) - (embedding_dim - 1) * delay\n",
    "    \n",
    "    for i in range(valid_length):\n",
    "        pattern = get_permutation_pattern(time_series, i, embedding_dim, delay)\n",
    "        pattern_counts[pattern] += 1\n",
    "    \n",
    "    pattern_probs = pattern_counts / valid_length\n",
    "    \n",
    "    # Calculate disequilibrium using Jensen-Shannon divergence\n",
    "    # Avoid zero probabilities\n",
    "    pattern_probs = np.clip(pattern_probs, 1e-10, 1.0)\n",
    "    disequilibrium = 0\n",
    "    \n",
    "    for prob in pattern_probs:\n",
    "        if prob > 0:\n",
    "            mean_prob = (prob + uniform_prob) / 2\n",
    "            disequilibrium += (prob * np.log(prob / mean_prob) + \n",
    "                             uniform_prob * np.log(uniform_prob / mean_prob)) / 2\n",
    "    \n",
    "    # Normalize disequilibrium\n",
    "    max_disequilibrium = -0.5 * np.log(1.0 / n_patterns)\n",
    "    disequilibrium = disequilibrium / max_disequilibrium\n",
    "    \n",
    "    # Calculate complexity as product of normalized entropy and disequilibrium\n",
    "    complexity = pe * disequilibrium\n",
    "    \n",
    "    return complexity\n",
    "\n",
    "# Function to process data for a single subject, activity, and axis\n",
    "def process_signal(signal, subject, activity, axis, embedded_dims, embedded_delays, signal_lengths):\n",
    "    \"\"\"\n",
    "    Process a signal and calculate permutation entropy and complexity.\n",
    "    \n",
    "    Args:\n",
    "        signal: Input signal data\n",
    "        subject: Subject identifier\n",
    "        activity: Activity label\n",
    "        axis: Axis label\n",
    "        embedded_dims: List of embedding dimensions\n",
    "        embedded_delays: List of embedding delays\n",
    "        signal_lengths: List of signal lengths\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for embedded_dim in embedded_dims:\n",
    "        for embedded_delay in embedded_delays:\n",
    "            for signal_length in signal_lengths:\n",
    "                # Skip if signal is too short\n",
    "                if len(signal) < signal_length:\n",
    "                    print(f\"Warning: Signal for {subject}, {activity}, {axis} has length {len(signal)} < {signal_length}\")\n",
    "                    continue\n",
    "                \n",
    "                # Use first signal_length points\n",
    "                signal_segment = signal[:signal_length]\n",
    "                \n",
    "                # Calculate metrics\n",
    "                try:\n",
    "                    pe = calculate_permutation_entropy(signal_segment, embedded_dim, embedded_delay)\n",
    "                    complexity = calculate_complexity(signal_segment, embedded_dim, embedded_delay)\n",
    "                    \n",
    "                    # Append results\n",
    "                    results.append({\n",
    "                        'subject': subject,\n",
    "                        'activity': activity,\n",
    "                        'axis': axis,\n",
    "                        'embedded_dim': embedded_dim,\n",
    "                        'embedded_delay': embedded_delay,\n",
    "                        'signal_length': signal_length,\n",
    "                        'permutation_entropy': pe,\n",
    "                        'complexity': complexity\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating metrics for {subject}, {activity}, {axis}, \" +\n",
    "                          f\"dim={embedded_dim}, delay={embedded_delay}, len={signal_length}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Function to visualize results\n",
    "def visualize_results(results_df, output_dir='permutation_entropy_results'):\n",
    "    \"\"\"\n",
    "    Generate visualizations for permutation entropy results.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with permutation entropy results\n",
    "        output_dir: Directory to save visualizations\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # ------ Visualization 1: Permutation entropy by activity and embedding dimension ------\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for activity in results_df['activity'].unique():\n",
    "        activity_data = results_df[results_df['activity'] == activity]\n",
    "        \n",
    "        # Average over axes, delays, signal lengths and subjects\n",
    "        avg_data = activity_data.groupby('embedded_dim')['permutation_entropy'].mean()\n",
    "        std_data = activity_data.groupby('embedded_dim')['permutation_entropy'].std()\n",
    "        \n",
    "        plt.errorbar(\n",
    "            avg_data.index, \n",
    "            avg_data.values, \n",
    "            yerr=std_data.values,\n",
    "            marker='o',\n",
    "            label=activity\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('Embedding Dimension')\n",
    "    plt.ylabel('Average Permutation Entropy')\n",
    "    plt.title('Permutation Entropy by Activity and Embedding Dimension')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'permutation_entropy_by_activity.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ------ Visualization 2: Complexity vs permutation entropy by activity ------\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for activity in results_df['activity'].unique():\n",
    "        activity_data = results_df[results_df['activity'] == activity]\n",
    "        \n",
    "        plt.scatter(\n",
    "            activity_data['permutation_entropy'], \n",
    "            activity_data['complexity'], \n",
    "            alpha=0.5,\n",
    "            label=activity\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('Permutation Entropy')\n",
    "    plt.ylabel('Complexity')\n",
    "    plt.title('Complexity vs Permutation Entropy by Activity')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'complexity_vs_entropy.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ------ Visualization 3: Permutation entropy by activity, axis, and embedding dimension ------\n",
    "    for axis in results_df['axis'].unique():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for activity in results_df['activity'].unique():\n",
    "            axis_activity_data = results_df[(results_df['activity'] == activity) & (results_df['axis'] == axis)]\n",
    "            \n",
    "            # Average over delays, signal lengths and subjects\n",
    "            avg_data = axis_activity_data.groupby('embedded_dim')['permutation_entropy'].mean()\n",
    "            std_data = axis_activity_data.groupby('embedded_dim')['permutation_entropy'].std()\n",
    "            \n",
    "            plt.errorbar(\n",
    "                avg_data.index, \n",
    "                avg_data.values, \n",
    "                yerr=std_data.values,\n",
    "                marker='o',\n",
    "                label=activity\n",
    "            )\n",
    "        \n",
    "        plt.xlabel('Embedding Dimension')\n",
    "        plt.ylabel('Average Permutation Entropy')\n",
    "        plt.title(f'Permutation Entropy by Activity (Axis: {axis})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.savefig(os.path.join(output_dir, f'permutation_entropy_axis_{axis}.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # ------ Visualization 4: Effect of embedding delay ------\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, delay in enumerate(sorted(results_df['embedded_delay'].unique())):\n",
    "        plt.subplot(1, len(results_df['embedded_delay'].unique()), i+1)\n",
    "        \n",
    "        for activity in results_df['activity'].unique():\n",
    "            delay_activity_data = results_df[(results_df['activity'] == activity) & \n",
    "                                            (results_df['embedded_delay'] == delay)]\n",
    "            \n",
    "            # Average over axes, signal lengths and subjects\n",
    "            avg_data = delay_activity_data.groupby('embedded_dim')['permutation_entropy'].mean()\n",
    "            \n",
    "            plt.plot(avg_data.index, avg_data.values, 'o-', label=activity)\n",
    "        \n",
    "        plt.xlabel('Embedding Dimension')\n",
    "        plt.ylabel('Average Permutation Entropy')\n",
    "        plt.title(f'Delay = {delay}')\n",
    "        if i == len(results_df['embedded_delay'].unique()) - 1:\n",
    "            plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'embedding_delay_effect.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ------ Visualization 5: Effect of signal length ------\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, length in enumerate(sorted(results_df['signal_length'].unique())):\n",
    "        plt.subplot(1, len(results_df['signal_length'].unique()), i+1)\n",
    "        \n",
    "        for activity in results_df['activity'].unique():\n",
    "            length_activity_data = results_df[(results_df['activity'] == activity) & \n",
    "                                             (results_df['signal_length'] == length)]\n",
    "            \n",
    "            # Average over axes, delays and subjects\n",
    "            avg_data = length_activity_data.groupby('embedded_dim')['permutation_entropy'].mean()\n",
    "            \n",
    "            plt.plot(avg_data.index, avg_data.values, 'o-', label=activity)\n",
    "        \n",
    "        plt.xlabel('Embedding Dimension')\n",
    "        plt.ylabel('Average Permutation Entropy')\n",
    "        plt.title(f'Signal Length = {length}')\n",
    "        if i == len(results_df['signal_length'].unique()) - 1:\n",
    "            plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'signal_length_effect.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ------ Visualization 6: Box plots of PE by activity ------\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    sns.boxplot(x='activity', y='permutation_entropy', data=results_df)\n",
    "    plt.xlabel('Activity')\n",
    "    plt.ylabel('Permutation Entropy')\n",
    "    plt.title('Distribution of Permutation Entropy by Activity')\n",
    "    plt.grid(True, axis='y')\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'permutation_entropy_boxplot.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ------ Visualization 7: Complexity distribution by activity ------\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    sns.boxplot(x='activity', y='complexity', data=results_df)\n",
    "    plt.xlabel('Activity')\n",
    "    plt.ylabel('Complexity')\n",
    "    plt.title('Distribution of Complexity by Activity')\n",
    "    plt.grid(True, axis='y')\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'complexity_boxplot.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ------ Visualization 8: Heatmap of average PE for different parameter combinations ------\n",
    "    dim_delay_data = results_df.groupby(['embedded_dim', 'embedded_delay'])['permutation_entropy'].mean().reset_index()\n",
    "    pivot_data = dim_delay_data.pivot('embedded_dim', 'embedded_delay', 'permutation_entropy')\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(pivot_data, annot=True, cmap='viridis', fmt='.3f')\n",
    "    plt.title('Average Permutation Entropy for Different Parameter Combinations')\n",
    "    plt.ylabel('Embedded Dimension')\n",
    "    plt.xlabel('Embedded Delay')\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'parameter_heatmap.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Function to generate sample data (for testing)\n",
    "def generate_sample_data():\n",
    "    \"\"\"\n",
    "    Generate sample accelerometer data for testing.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with sample data\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Define parameters\n",
    "    subjects = range(1, 16)  # 15 subjects\n",
    "    activities = ['walking', 'running', 'climbing_up', 'climbing_down']\n",
    "    axes = ['x', 'y', 'z']\n",
    "    sample_length = 5000\n",
    "    \n",
    "    # Generate sample data\n",
    "    data = []\n",
    "    \n",
    "    for subject in subjects:\n",
    "        for activity in activities:\n",
    "            for axis in axes:\n",
    "                # Generate different patterns based on activity\n",
    "                if activity == 'walking':\n",
    "                    # Walking: periodic with moderate noise\n",
    "                    t = np.linspace(0, 10, sample_length)\n",
    "                    signal = np.sin(2 * np.pi * t) + 0.2 * np.random.normal(size=sample_length)\n",
    "                elif activity == 'running':\n",
    "                    # Running: faster periodic with higher amplitude and noise\n",
    "                    t = np.linspace(0, 10, sample_length)\n",
    "                    signal = 1.5 * np.sin(4 * np.pi * t) + 0.4 * np.random.normal(size=sample_length)\n",
    "                elif activity == 'climbing_up':\n",
    "                    # Climbing up: asymmetric pattern\n",
    "                    t = np.linspace(0, 10, sample_length)\n",
    "                    signal = np.sin(2 * np.pi * t) + 0.5 * np.sin(4 * np.pi * t) + 0.3 * np.random.normal(size=sample_length)\n",
    "                else:  # climbing_down\n",
    "                    # Climbing down: different asymmetric pattern\n",
    "                    t = np.linspace(0, 10, sample_length)\n",
    "                    signal = np.sin(2 * np.pi * t) - 0.5 * np.sin(4 * np.pi * t) + 0.3 * np.random.normal(size=sample_length)\n",
    "                \n",
    "                # Add axis-specific modifications\n",
    "                if axis == 'x':\n",
    "                    signal = signal * 1.2\n",
    "                elif axis == 'y':\n",
    "                    signal = signal * 0.8 + 0.5\n",
    "                else:  # z axis\n",
    "                    signal = signal * 1.0 - 0.2\n",
    "                \n",
    "                # Add subject-specific noise\n",
    "                subject_noise = 0.1 * (subject / 15) * np.random.normal(size=sample_length)\n",
    "                signal = signal + subject_noise\n",
    "                \n",
    "                # Process signal and calculate PE and complexity\n",
    "                subject_id = f'subject_{subject}'\n",
    "                embedded_dims = [3, 4, 5, 6]\n",
    "                embedded_delays = [1, 2, 3]\n",
    "                signal_lengths = [1024, 2048, 4096]\n",
    "                \n",
    "                results = process_signal(\n",
    "                    signal, subject_id, activity, axis, \n",
    "                    embedded_dims, embedded_delays, signal_lengths\n",
    "                )\n",
    "                \n",
    "                # Add to data\n",
    "                data.append(results)\n",
    "    \n",
    "    # Combine all results\n",
    "    combined_data = pd.concat(data, ignore_index=True)\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the permutation entropy analysis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ask the user whether to use sample data or load from disk\n",
    "        print(\"Permutation Entropy and Complexity Analysis\")\n",
    "        print(\"------------------------------------------\")\n",
    "        print(\"1. Use sample generated data (for testing)\")\n",
    "        print(\"2. Load processed data from CSV file\")\n",
    "        choice = input(\"Enter your choice (1/2): \")\n",
    "        \n",
    "        results_df = None\n",
    "        \n",
    "        if choice == '1':\n",
    "            print(\"Generating sample data...\")\n",
    "            results_df = generate_sample_data()\n",
    "        elif choice == '2':\n",
    "            file_path = input(\"Enter the path to the CSV file: \")\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"Loading data from {file_path}...\")\n",
    "                results_df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Check if the loaded data has the required columns\n",
    "                required_columns = ['subject', 'activity', 'axis', 'embedded_dim', \n",
    "                                    'embedded_delay', 'signal_length', \n",
    "                                    'permutation_entropy', 'complexity']\n",
    "                \n",
    "                missing_columns = [col for col in required_columns if col not in results_df.columns]\n",
    "                \n",
    "                if missing_columns:\n",
    "                    print(f\"Warning: Loaded data is missing columns: {missing_columns}\")\n",
    "                    print(\"Cannot proceed with visualization.\")\n",
    "                    return\n",
    "            else:\n",
    "                print(f\"Error: File not found at {file_path}\")\n",
    "                return\n",
    "        else:\n",
    "            print(\"Invalid choice. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        if results_df is not None and not results_df.empty:\n",
    "            # Get summary statistics\n",
    "            print(\"\\nSummary Statistics:\")\n",
    "            print(f\"Number of samples: {len(results_df)}\")\n",
    "            print(f\"Number of subjects: {len(results_df['subject'].unique())}\")\n",
    "            print(f\"Activities: {results_df['activity'].unique()}\")\n",
    "            print(f\"Axes: {results_df['axis'].unique()}\")\n",
    "            print(f\"Embedding dimensions: {sorted(results_df['embedded_dim'].unique())}\")\n",
    "            print(f\"Embedding delays: {sorted(results_df['embedded_delay'].unique())}\")\n",
    "            print(f\"Signal lengths: {sorted(results_df['signal_length'].unique())}\")\n",
    "            \n",
    "            # Average PE and complexity by activity\n",
    "            activity_stats = results_df.groupby('activity')[['permutation_entropy', 'complexity']].agg(['mean', 'std'])\n",
    "            print(\"\\nPermutation Entropy and Complexity by Activity:\")\n",
    "            print(activity_stats)\n",
    "            \n",
    "            # Generate visualizations\n",
    "            print(\"\\nGenerating visualizations...\")\n",
    "            visualize_results(results_df)\n",
    "            \n",
    "            print(\"\\nAnalysis complete. Results saved to 'permutation_entropy_results' directory.\")\n",
    "        else:\n",
    "            print(\"No data to analyze.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://drive.google.com/drive/folders/1JneWV9NL1v2R8LpiTNKw2oqTgd3o2Sgx?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf073b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation Entropy and Complexity Analysis\n",
      "------------------------------------------\n",
      "1. Use sample generated data (for testing)\n",
      "2. Load processed data from CSV file\n",
      "Enter your choice (1/2): 2\n",
      "Enter the path to the CSV file: processed_permutation_entropy_complexity.csv\n",
      "Loading data from processed_permutation_entropy_complexity.csv...\n",
      "Warning: Loaded data is missing columns: ['subject', 'activity', 'axis', 'embedded_dim', 'embedded_delay', 'signal_length', 'permutation_entropy', 'complexity']\n",
      "Cannot proceed with visualization.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Function to calculate permutation pattern\n",
    "def get_permutation_pattern(time_series, start_idx, embedding_dim, delay):\n",
    "    \"\"\"\n",
    "    Extract the permutation pattern from a time series segment.\n",
    "    \n",
    "    Args:\n",
    "        time_series: Input time series data\n",
    "        start_idx: Starting index\n",
    "        embedding_dim: Embedding dimension\n",
    "        delay: Time delay\n",
    "        \n",
    "    Returns:\n",
    "        Pattern index representing the ordinal pattern\n",
    "    \"\"\"\n",
    "    # Extract values for the pattern\n",
    "    pattern_values = [time_series[start_idx + i * delay] for i in range(embedding_dim)]\n",
    "    \n",
    "    # Get the indices that would sort the pattern\n",
    "    sorted_indices = np.argsort(pattern_values)\n",
    "    \n",
    "    # Convert to a single integer representation (factorial number system)\n",
    "    pattern_index = 0\n",
    "    base = 1\n",
    "    for i in range(embedding_dim - 1, -1, -1):\n",
    "        pattern_index += sorted_indices[i] * base\n",
    "        base *= (i + 1)\n",
    "    \n",
    "    return pattern_index\n",
    "\n",
    "# Function to calculate permutation entropy\n",
    "def calculate_permutation_entropy(time_series, embedding_dim, delay, normalize=True):\n",
    "    \"\"\"\n",
    "    Calculate permutation entropy for a time series.\n",
    "    \n",
    "    Args:\n",
    "        time_series: Input time series data\n",
    "        embedding_dim: Embedding dimension\n",
    "        delay: Time delay\n",
    "        normalize: Whether to normalize the entropy value\n",
    "        \n",
    "    Returns:\n",
    "        Permutation entropy value\n",
    "    \"\"\"\n",
    "    # Ensure time series is NumPy array\n",
    "    time_series = np.array(time_series)\n",
    "    \n",
    "    # Calculate the number of patterns possible\n",
    "    n_patterns = math.factorial(embedding_dim)\n",
    "    \n",
    "    # Array to store pattern counts\n",
    "    pattern_counts = np.zeros(n_patterns)\n",
    "    \n",
    "    # Calculate valid length for patterns\n",
    "    valid_length = len(time_series) - (embedding_dim - 1) * delay\n",
    "    \n",
    "    # Count patterns\n",
    "    for i in range(valid_length):\n",
    "        pattern = get_permutation_pattern(time_series, i, embedding_dim, delay)\n",
    "        pattern_counts[pattern] += 1\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    pattern_probs = pattern_counts / valid_length\n",
    "    \n",
    "    # Remove zero probabilities for entropy calculation\n",
    "    pattern_probs = pattern_probs[pattern_probs > 0]\n",
    "    \n",
    "    # Calculate entropy\n",
    "    pe = entropy(pattern_probs, base=math.e)\n",
    "    \n",
    "    # Normalize if requested\n",
    "    if normalize and pe != 0:\n",
    "        pe = pe / np.log(n_patterns)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# Function to calculate complexity based on permutation entropy\n",
    "def calculate_complexity(time_series, embedding_dim, delay):\n",
    "    \"\"\"\n",
    "    Calculate statistical complexity based on permutation entropy.\n",
    "    \n",
    "    Args:\n",
    "        time_series: Input time series data\n",
    "        embedding_dim: Embedding dimension\n",
    "        delay: Time delay\n",
    "        \n",
    "    Returns:\n",
    "        Complexity value\n",
    "    \"\"\"\n",
    "    # Calculate permutation entropy (normalized)\n",
    "    pe = calculate_permutation_entropy(time_series, embedding_dim, delay, normalize=True)\n",
    "    \n",
    "    # Calculate disequilibrium (using Jensen-Shannon divergence approximation)\n",
    "    n_patterns = math.factorial(embedding_dim)\n",
    "    \n",
    "    # Uniform distribution (maximum entropy)\n",
    "    uniform_prob = 1.0 / n_patterns\n",
    "    \n",
    "    # Calculate pattern probabilities\n",
    "    pattern_counts = np.zeros(n_patterns)\n",
    "    valid_length = len(time_series) - (embedding_dim - 1) * delay\n",
    "    \n",
    "    for i in range(valid_length):\n",
    "        pattern = get_permutation_pattern(time_series, i, embedding_dim, delay)\n",
    "        pattern_counts[pattern] += 1\n",
    "    \n",
    "    pattern_probs = pattern_counts / valid_length\n",
    "    \n",
    "    # Calculate disequilibrium using Jensen-Shannon divergence\n",
    "    # Avoid zero probabilities\n",
    "    pattern_probs = np.clip(pattern_probs, 1e-10, 1.0)\n",
    "    disequilibrium = 0\n",
    "    \n",
    "    for prob in pattern_probs:\n",
    "        if prob > 0:\n",
    "            mean_prob = (prob + uniform_prob) / 2\n",
    "            disequilibrium += (prob * np.log(prob / mean_prob) + \n",
    "                             uniform_prob * np.log(uniform_prob / mean_prob)) / 2\n",
    "    \n",
    "    # Normalize disequilibrium\n",
    "    max_disequilibrium = -0.5 * np.log(1.0 / n_patterns)\n",
    "    disequilibrium = disequilibrium / max_disequilibrium\n",
    "    \n",
    "    # Calculate complexity as product of normalized entropy and disequilibrium\n",
    "    complexity = pe * disequilibrium\n",
    "    \n",
    "    return complexity\n",
    "\n",
    "# Function to process data for a single subject, activity, and axis\n",
    "def process_signal(signal, subject, activity, axis, embedded_dims, embedded_delays, signal_lengths):\n",
    "    \"\"\"\n",
    "    Process a signal and calculate permutation entropy and complexity.\n",
    "    \n",
    "    Args:\n",
    "        signal: Input signal data\n",
    "        subject: Subject identifier\n",
    "        activity: Activity label\n",
    "        axis: Axis label\n",
    "        embedded_dims: List of embedding dimensions\n",
    "        embedded_delays: List of embedding delays\n",
    "        signal_lengths: List of signal lengths\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for embedded_dim in embedded_dims:\n",
    "        for embedded_delay in embedded_delays:\n",
    "            for signal_length in signal_lengths:\n",
    "                # Skip if signal is too short\n",
    "                if len(signal) < signal_length:\n",
    "                    print(f\"Warning: Signal for {subject}, {activity}, {axis} has length {len(signal)} < {signal_length}\")\n",
    "                    continue\n",
    "                \n",
    "                # Use first signal_length points\n",
    "                signal_segment = signal[:signal_length]\n",
    "                \n",
    "                # Calculate metrics\n",
    "                try:\n",
    "                    pe = calculate_permutation_entropy(signal_segment, embedded_dim, embedded_delay)\n",
    "                    complexity = calculate_complexity(signal_segment, embedded_dim, embedded_delay)\n",
    "                    \n",
    "                    # Append results\n",
    "                    results.append({\n",
    "                        'subject': subject,\n",
    "                        'activity': activity,\n",
    "                        'axis': axis,\n",
    "                        'embedded_dim': embedded_dim,\n",
    "                        'embedded_delay': embedded_delay,\n",
    "                        'signal_length': signal_length,\n",
    "                        'permutation_entropy': pe,\n",
    "                        'complexity': complexity\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating metrics for {subject}, {activity}, {axis}, \" +\n",
    "                          f\"dim={embedded_dim}, delay={embedded_delay}, len={signal_length}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Function to visualize results\n",
    "def visualize_results(results_df, output_dir='permutation_entropy_results'):\n",
    "    \"\"\"\n",
    "    Generate visualizations for permutation entropy results.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with permutation entropy results\n",
    "        output_dir: Directory to save visualizations\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # ------ Visualization 1: Permutation entropy by activity and embedding dimension ------\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for activity in results_df['activity'].unique():\n",
    "        activity_data = results_df[results_df['activity'] == activity]\n",
    "        \n",
    "        # Average over axes, delays, signal lengths and subjects\n",
    "        avg_data = activity_data.groupby('embedded_dim')['permutation_entropy'].mean()\n",
    "        std_data = activity_data.groupby('embedded_dim')['permutation_entropy'].std()\n",
    "        \n",
    "        plt.errorbar(\n",
    "            avg_data.index, \n",
    "            avg_data.values, \n",
    "            yerr=std_data.values,\n",
    "            marker='o',\n",
    "            label=activity\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('Embedding Dimension')\n",
    "    plt.ylabel('Average Permutation Entropy')\n",
    "    plt.title('Permutation Entropy by Activity and Embedding Dimension')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'permutation_entropy_by_activity.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ------ Visualization 2: Complexity vs permutation entropy by activity ------\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for activity in results_df['activity'].unique():\n",
    "        activity_data = results_df[results_df['activity'] == activity]\n",
    "        \n",
    "        plt.scatter(\n",
    "            activity_data['permutation_entropy'], \n",
    "            activity_data['complexity'], \n",
    "            alpha=0.5,\n",
    "            label=activity\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('Permutation Entropy')\n",
    "    plt.ylabel('Complexity')\n",
    "    plt.title('Complexity vs Permutation Entropy by Activity')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'complexity_vs_entropy.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ------ Visualization 3: Permutation entropy by activity, axis, and embedding dimension ------\n",
    "    for axis in results_df['axis'].unique():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for activity in results_df['activity'].unique():\n",
    "            axis_activity_data = results_df[(results_df['activity'] == activity) & (results_df['axis'] == axis)]\n",
    "            \n",
    "            # Average over delays, signal lengths and subjects\n",
    "            avg_data = axis_activity_data.groupby('embedded_dim')['permutation_entropy'].mean()\n",
    "            std_data = axis_activity_data.groupby('embedded_dim')['permutation_entropy'].std()\n",
    "            \n",
    "            plt.errorbar(\n",
    "                avg_data.index, \n",
    "                avg_data.values, \n",
    "                yerr=std_data.values,\n",
    "                marker='o',\n",
    "                label=activity\n",
    "            )\n",
    "        \n",
    "        plt.xlabel('Embedding Dimension')\n",
    "        plt.ylabel('Average Permutation Entropy')\n",
    "        plt.title(f'Permutation Entropy by Activity (Axis: {axis})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.savefig(os.path.join(output_dir, f'permutation_entropy_axis_{axis}.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # ------ Visualization 4: Effect of embedding delay ------\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, delay in enumerate(sorted(results_df['embedded_delay'].unique())):\n",
    "        plt.subplot(1, len(results_df['embedded_delay'].unique()), i+1)\n",
    "        \n",
    "        for activity in results_df['activity'].unique():\n",
    "            delay_activity_data = results_df[(results_df['activity'] == activity) & \n",
    "                                            (results_df['embedded_delay'] == delay)]\n",
    "            \n",
    "            # Average over axes, signal lengths and subjects\n",
    "            avg_data = delay_activity_data.groupby('embedded_dim')['permutation_entropy'].mean()\n",
    "            \n",
    "            plt.plot(avg_data.index, avg_data.values, 'o-', label=activity)\n",
    "        \n",
    "        plt.xlabel('Embedding Dimension')\n",
    "        plt.ylabel('Average Permutation Entropy')\n",
    "        plt.title(f'Delay = {delay}')\n",
    "        if i == len(results_df['embedded_delay'].unique()) - 1:\n",
    "            plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'embedding_delay_effect.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ------ Visualization 5: Effect of signal length ------\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, length in enumerate(sorted(results_df['signal_length'].unique())):\n",
    "        plt.subplot(1, len(results_df['signal_length'].unique()), i+1)\n",
    "        \n",
    "        for activity in results_df['activity'].unique():\n",
    "            length_activity_data = results_df[(results_df['activity'] == activity) & \n",
    "                                             (results_df['signal_length'] == length)]\n",
    "            \n",
    "            # Average over axes, delays and subjects\n",
    "            avg_data = length_activity_data.groupby('embedded_dim')['permutation_entropy'].mean()\n",
    "            \n",
    "            plt.plot(avg_data.index, avg_data.values, 'o-', label=activity)\n",
    "        \n",
    "        plt.xlabel('Embedding Dimension')\n",
    "        plt.ylabel('Average Permutation Entropy')\n",
    "        plt.title(f'Signal Length = {length}')\n",
    "        if i == len(results_df['signal_length'].unique()) - 1:\n",
    "            plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'signal_length_effect.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ------ Visualization 6: Box plots of PE by activity ------\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    sns.boxplot(x='activity', y='permutation_entropy', data=results_df)\n",
    "    plt.xlabel('Activity')\n",
    "    plt.ylabel('Permutation Entropy')\n",
    "    plt.title('Distribution of Permutation Entropy by Activity')\n",
    "    plt.grid(True, axis='y')\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'permutation_entropy_boxplot.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ------ Visualization 7: Complexity distribution by activity ------\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    sns.boxplot(x='activity', y='complexity', data=results_df)\n",
    "    plt.xlabel('Activity')\n",
    "    plt.ylabel('Complexity')\n",
    "    plt.title('Distribution of Complexity by Activity')\n",
    "    plt.grid(True, axis='y')\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'complexity_boxplot.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ------ Visualization 8: Heatmap of average PE for different parameter combinations ------\n",
    "    dim_delay_data = results_df.groupby(['embedded_dim', 'embedded_delay'])['permutation_entropy'].mean().reset_index()\n",
    "    pivot_data = dim_delay_data.pivot('embedded_dim', 'embedded_delay', 'permutation_entropy')\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(pivot_data, annot=True, cmap='viridis', fmt='.3f')\n",
    "    plt.title('Average Permutation Entropy for Different Parameter Combinations')\n",
    "    plt.ylabel('Embedded Dimension')\n",
    "    plt.xlabel('Embedded Delay')\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, 'parameter_heatmap.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Function to generate sample data (for testing)\n",
    "def generate_sample_data():\n",
    "    \"\"\"\n",
    "    Generate sample accelerometer data for testing.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with sample data\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Define parameters\n",
    "    subjects = range(1, 16)  # 15 subjects\n",
    "    activities = ['walking', 'running', 'climbing_up', 'climbing_down']\n",
    "    axes = ['x', 'y', 'z']\n",
    "    sample_length = 5000\n",
    "    \n",
    "    # Generate sample data\n",
    "    data = []\n",
    "    \n",
    "    for subject in subjects:\n",
    "        for activity in activities:\n",
    "            for axis in axes:\n",
    "                # Generate different patterns based on activity\n",
    "                if activity == 'walking':\n",
    "                    # Walking: periodic with moderate noise\n",
    "                    t = np.linspace(0, 10, sample_length)\n",
    "                    signal = np.sin(2 * np.pi * t) + 0.2 * np.random.normal(size=sample_length)\n",
    "                elif activity == 'running':\n",
    "                    # Running: faster periodic with higher amplitude and noise\n",
    "                    t = np.linspace(0, 10, sample_length)\n",
    "                    signal = 1.5 * np.sin(4 * np.pi * t) + 0.4 * np.random.normal(size=sample_length)\n",
    "                elif activity == 'climbing_up':\n",
    "                    # Climbing up: asymmetric pattern\n",
    "                    t = np.linspace(0, 10, sample_length)\n",
    "                    signal = np.sin(2 * np.pi * t) + 0.5 * np.sin(4 * np.pi * t) + 0.3 * np.random.normal(size=sample_length)\n",
    "                else:  # climbing_down\n",
    "                    # Climbing down: different asymmetric pattern\n",
    "                    t = np.linspace(0, 10, sample_length)\n",
    "                    signal = np.sin(2 * np.pi * t) - 0.5 * np.sin(4 * np.pi * t) + 0.3 * np.random.normal(size=sample_length)\n",
    "                \n",
    "                # Add axis-specific modifications\n",
    "                if axis == 'x':\n",
    "                    signal = signal * 1.2\n",
    "                elif axis == 'y':\n",
    "                    signal = signal * 0.8 + 0.5\n",
    "                else:  # z axis\n",
    "                    signal = signal * 1.0 - 0.2\n",
    "                \n",
    "                # Add subject-specific noise\n",
    "                subject_noise = 0.1 * (subject / 15) * np.random.normal(size=sample_length)\n",
    "                signal = signal + subject_noise\n",
    "                \n",
    "                # Process signal and calculate PE and complexity\n",
    "                subject_id = f'subject_{subject}'\n",
    "                embedded_dims = [3, 4, 5, 6]\n",
    "                embedded_delays = [1, 2, 3]\n",
    "                signal_lengths = [1024, 2048, 4096]\n",
    "                \n",
    "                results = process_signal(\n",
    "                    signal, subject_id, activity, axis, \n",
    "                    embedded_dims, embedded_delays, signal_lengths\n",
    "                )\n",
    "                \n",
    "                # Add to data\n",
    "                data.append(results)\n",
    "    \n",
    "    # Combine all results\n",
    "    combined_data = pd.concat(data, ignore_index=True)\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the permutation entropy analysis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ask the user whether to use sample data or load from disk\n",
    "        print(\"Permutation Entropy and Complexity Analysis\")\n",
    "        print(\"------------------------------------------\")\n",
    "        print(\"1. Use sample generated data (for testing)\")\n",
    "        print(\"2. Load processed data from CSV file\")\n",
    "        choice = input(\"Enter your choice (1/2): \")\n",
    "        \n",
    "        results_df = None\n",
    "        \n",
    "        if choice == '1':\n",
    "            print(\"Generating sample data...\")\n",
    "            results_df = generate_sample_data()\n",
    "        elif choice == '2':\n",
    "            file_path = input(\"Enter the path to the CSV file: \")\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"Loading data from {file_path}...\")\n",
    "                results_df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Check if the loaded data has the required columns\n",
    "                required_columns = ['subject', 'activity', 'axis', 'embedded_dim', \n",
    "                                    'embedded_delay', 'signal_length', \n",
    "                                    'permutation_entropy', 'complexity']\n",
    "                \n",
    "                missing_columns = [col for col in required_columns if col not in results_df.columns]\n",
    "                \n",
    "                if missing_columns:\n",
    "                    print(f\"Warning: Loaded data is missing columns: {missing_columns}\")\n",
    "                    print(\"Cannot proceed with visualization.\")\n",
    "                    return\n",
    "            else:\n",
    "                print(f\"Error: File not found at {file_path}\")\n",
    "                return\n",
    "        else:\n",
    "            print(\"Invalid choice. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        if results_df is not None and not results_df.empty:\n",
    "            # Get summary statistics\n",
    "            print(\"\\nSummary Statistics:\")\n",
    "            print(f\"Number of samples: {len(results_df)}\")\n",
    "            print(f\"Number of subjects: {len(results_df['subject'].unique())}\")\n",
    "            print(f\"Activities: {results_df['activity'].unique()}\")\n",
    "            print(f\"Axes: {results_df['axis'].unique()}\")\n",
    "            print(f\"Embedding dimensions: {sorted(results_df['embedded_dim'].unique())}\")\n",
    "            print(f\"Embedding delays: {sorted(results_df['embedded_delay'].unique())}\")\n",
    "            print(f\"Signal lengths: {sorted(results_df['signal_length'].unique())}\")\n",
    "            \n",
    "            # Average PE and complexity by activity\n",
    "            activity_stats = results_df.groupby('activity')[['permutation_entropy', 'complexity']].agg(['mean', 'std'])\n",
    "            print(\"\\nPermutation Entropy and Complexity by Activity:\")\n",
    "            print(activity_stats)\n",
    "            \n",
    "            # Generate visualizations\n",
    "            print(\"\\nGenerating visualizations...\")\n",
    "            visualize_results(results_df)\n",
    "            \n",
    "            print(\"\\nAnalysis complete. Results saved to 'permutation_entropy_results' directory.\")\n",
    "        else:\n",
    "            print(\"No data to analyze.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e0647e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95f0c3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking /Users/rosalinatorres/Downloads...\n",
      "Found 135 CSV files in /Users/rosalinatorres/Downloads:\n",
      "  - /Users/rosalinatorres/Downloads/processed_permutation_entropy_complexity.csv\n",
      "  - /Users/rosalinatorres/Downloads/Pandas-Data-Science-Tasks-master/SalesAnalysis/Output/all_data.csv\n",
      "  - /Users/rosalinatorres/Downloads/Pandas-Data-Science-Tasks-master/SalesAnalysis/Sales_Data/Sales_December_2019.csv\n",
      "  - /Users/rosalinatorres/Downloads/Pandas-Data-Science-Tasks-master/SalesAnalysis/Sales_Data/Sales_April_2019.csv\n",
      "  - /Users/rosalinatorres/Downloads/Pandas-Data-Science-Tasks-master/SalesAnalysis/Sales_Data/Sales_February_2019.csv\n",
      "  ... and 130 more files\n",
      "Checking /Users/rosalinatorres/Documents...\n",
      "Found 1 CSV files in /Users/rosalinatorres/Documents:\n",
      "  - /Users/rosalinatorres/Documents/processed_permutation_entropy_complexity.csv\n",
      "Checking /Users/rosalinatorres/Desktop...\n",
      "Found 31 CSV files in /Users/rosalinatorres/Desktop:\n",
      "  - /Users/rosalinatorres/Desktop/Taxes/myenv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-log2.csv\n",
      "  - /Users/rosalinatorres/Desktop/Taxes/myenv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-arcsinh.csv\n",
      "  - /Users/rosalinatorres/Desktop/Taxes/myenv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-arctanh.csv\n",
      "  - /Users/rosalinatorres/Desktop/Taxes/myenv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-sin.csv\n",
      "  - /Users/rosalinatorres/Desktop/Taxes/myenv/lib/python3.13/site-packages/numpy/_core/tests/data/umath-validation-set-cos.csv\n",
      "  ... and 26 more files\n",
      "Checking /Users/rosalinatorres/democracy-clustering-analysis...\n",
      "Found 3 CSV files in /Users/rosalinatorres/democracy-clustering-analysis:\n",
      "  - /Users/rosalinatorres/democracy-clustering-analysis/permutation_entropy_results.csv\n",
      "  - /Users/rosalinatorres/democracy-clustering-analysis/permutation_entropy_complexity_results.csv\n",
      "  - /Users/rosalinatorres/democracy-clustering-analysis/processed_permutation_entropy_complexity.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Common places to check\n",
    "potential_locations = [\n",
    "    os.path.expanduser(\"~/Downloads\"),\n",
    "    os.path.expanduser(\"~/Documents\"),\n",
    "    os.path.expanduser(\"~/Desktop\"),\n",
    "    os.getcwd()  # Current working directory\n",
    "]\n",
    "\n",
    "# Search for CSV files\n",
    "for location in potential_locations:\n",
    "    if os.path.exists(location):\n",
    "        print(f\"Checking {location}...\")\n",
    "        csv_files = glob.glob(os.path.join(location, \"**/*.csv\"), recursive=True)\n",
    "        if csv_files:\n",
    "            print(f\"Found {len(csv_files)} CSV files in {location}:\")\n",
    "            for file in csv_files[:5]:  # Show first 5 files\n",
    "                print(f\"  - {file}\")\n",
    "            if len(csv_files) > 5:\n",
    "                print(f\"  ... and {len(csv_files) - 5} more files\")\n",
    "        else:\n",
    "            print(f\"No CSV files found in {location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8aabed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Files in this directory:\", os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43e7975e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /Applications/anaconda3/lib/python3.12/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Applications/anaconda3/lib/python3.12/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/lib/python3.12/site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in /Applications/anaconda3/lib/python3.12/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/lib/python3.12/site-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Applications/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/lib/python3.12/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/lib/python3.12/site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/lib/python3.12/site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.12/site-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Applications/anaconda3/lib/python3.12/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b805b5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flexible Permutation Entropy Analysis\n",
      "------------------------------------\n",
      "Enter the path to the CSV file: /Users/rosalinatorres/Downloads/project2_data_chest\n",
      "Error analyzing CSV file: [Errno 21] Is a directory: '/Users/rosalinatorres/Downloads/project2_data_chest'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/7d/38n8qyc974dfs9nc462v_ln40000gn/T/ipykernel_12384/3618087377.py\", line 24, in analyze_csv_file\n",
      "    df = pd.read_csv(file_path)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ^^^^^^^^^^^\n",
      "  File \"/Applications/anaconda3/lib/python3.12/site-packages/pandas/io/common.py\", line 873, in get_handle\n",
      "    handle = open(\n",
      "             ^^^^^\n",
      "IsADirectoryError: [Errno 21] Is a directory: '/Users/rosalinatorres/Downloads/project2_data_chest'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def analyze_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Analyze and visualize data from a CSV file with flexible column mapping.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: File not found at {file_path}\")\n",
    "            return\n",
    "        \n",
    "        # Try to load the file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Basic information\n",
    "        print(f\"\\nCSV File Analysis: {file_path}\")\n",
    "        print(f\"Number of rows: {len(df)}\")\n",
    "        print(f\"Number of columns: {len(df.columns)}\")\n",
    "        print(\"\\nColumn names:\")\n",
    "        for i, col in enumerate(df.columns):\n",
    "            print(f\"  {i+1}. {col}\")\n",
    "        \n",
    "        # Show first few rows\n",
    "        print(\"\\nFirst 3 rows:\")\n",
    "        print(df.head(3).to_string())\n",
    "        \n",
    "        # Check numeric columns for potentially relevant data\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        print(f\"\\nNumeric columns (potential entropy/complexity values): {numeric_cols}\")\n",
    "        \n",
    "        # Check for categorical columns\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        print(f\"\\nCategorical columns (potential activity/subject labels): {categorical_cols}\")\n",
    "        \n",
    "        # For small number of columns, try to guess mappings\n",
    "        if len(df.columns) < 20:\n",
    "            print(\"\\nAttempting to guess column mappings...\")\n",
    "            guessed_mappings = guess_column_mappings(df)\n",
    "            for expected, guessed in guessed_mappings.items():\n",
    "                if guessed:\n",
    "                    print(f\"  '{expected}' might be '{guessed}'\")\n",
    "                else:\n",
    "                    print(f\"  No clear match for '{expected}'\")\n",
    "            \n",
    "            # Ask if the user wants to use the guessed mappings or define their own\n",
    "            print(\"\\nDo you want to:\")\n",
    "            print(\"1. Use these guessed mappings\")\n",
    "            print(\"2. Define your own mappings\")\n",
    "            print(\"3. Proceed without mappings (try to use columns as-is)\")\n",
    "            choice = input(\"Enter your choice (1/2/3): \")\n",
    "            \n",
    "            if choice == '1':\n",
    "                # Use guessed mappings\n",
    "                column_mapping = {v: k for k, v in guessed_mappings.items() if v}\n",
    "                mapped_df = apply_column_mapping(df, column_mapping)\n",
    "                generate_visualizations(mapped_df)\n",
    "            elif choice == '2':\n",
    "                # Define custom mappings\n",
    "                column_mapping = define_custom_mappings(df)\n",
    "                mapped_df = apply_column_mapping(df, column_mapping)\n",
    "                generate_visualizations(mapped_df)\n",
    "            else:\n",
    "                # Try to use columns as-is\n",
    "                print(\"\\nProceeding with direct analysis...\")\n",
    "                try_direct_analysis(df)\n",
    "        else:\n",
    "            # Too many columns to guess, ask user to define mappings\n",
    "            print(\"\\nToo many columns to guess mappings automatically.\")\n",
    "            print(\"Do you want to define your own mappings? (y/n)\")\n",
    "            choice = input(\"Enter y/n: \")\n",
    "            \n",
    "            if choice.lower() == 'y':\n",
    "                column_mapping = define_custom_mappings(df)\n",
    "                mapped_df = apply_column_mapping(df, column_mapping)\n",
    "                generate_visualizations(mapped_df)\n",
    "            else:\n",
    "                # Try to use columns as-is\n",
    "                print(\"\\nProceeding with direct analysis...\")\n",
    "                try_direct_analysis(df)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing CSV file: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def guess_column_mappings(df):\n",
    "    \"\"\"\n",
    "    Attempt to guess column mappings based on column names and data properties.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of guessed mappings (expected_column: guessed_column)\n",
    "    \"\"\"\n",
    "    expected_columns = [\n",
    "        'subject', 'activity', 'axis', \n",
    "        'embedded_dim', 'embedded_delay', 'signal_length', \n",
    "        'permutation_entropy', 'complexity'\n",
    "    ]\n",
    "    \n",
    "    guessed_mappings = {col: None for col in expected_columns}\n",
    "    \n",
    "    # Look for each expected column based on name similarity\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        \n",
    "        # Subject\n",
    "        if 'subject' in col_lower or 'subj' in col_lower or 'person' in col_lower or 'individual' in col_lower:\n",
    "            guessed_mappings['subject'] = col\n",
    "        \n",
    "        # Activity\n",
    "        elif 'activity' in col_lower or 'action' in col_lower or 'movement' in col_lower or 'motion' in col_lower:\n",
    "            guessed_mappings['activity'] = col\n",
    "        \n",
    "        # Axis\n",
    "        elif 'axis' in col_lower or 'dimension' in col_lower or ('x' == col_lower or 'y' == col_lower or 'z' == col_lower):\n",
    "            guessed_mappings['axis'] = col\n",
    "        \n",
    "        # Embedded dimension\n",
    "        elif ('embedded' in col_lower and 'dim' in col_lower) or 'embed_dim' in col_lower or 'embdim' in col_lower:\n",
    "            guessed_mappings['embedded_dim'] = col\n",
    "        \n",
    "        # Embedded delay\n",
    "        elif ('embedded' in col_lower and 'delay' in col_lower) or 'embed_delay' in col_lower or 'tau' in col_lower:\n",
    "            guessed_mappings['embedded_delay'] = col\n",
    "        \n",
    "        # Signal length\n",
    "        elif ('signal' in col_lower and 'length' in col_lower) or 'window' in col_lower or 'size' in col_lower:\n",
    "            guessed_mappings['signal_length'] = col\n",
    "        \n",
    "        # Permutation entropy\n",
    "        elif ('permutation' in col_lower and 'entropy' in col_lower) or 'pe' == col_lower or 'perm_ent' in col_lower:\n",
    "            guessed_mappings['permutation_entropy'] = col\n",
    "        \n",
    "        # Complexity\n",
    "        elif 'complexity' in col_lower or 'complex' == col_lower:\n",
    "            guessed_mappings['complexity'] = col\n",
    "    \n",
    "    # If we didn't find matches by name, try to guess based on data properties\n",
    "    # Look for numeric columns that might be parameters\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # For embedded_dim, look for columns with small integers (3-6)\n",
    "    if guessed_mappings['embedded_dim'] is None and numeric_cols:\n",
    "        for col in numeric_cols:\n",
    "            unique_values = sorted(df[col].unique())\n",
    "            if len(unique_values) <= 4 and set(unique_values).issubset({3, 4, 5, 6}):\n",
    "                guessed_mappings['embedded_dim'] = col\n",
    "                break\n",
    "    \n",
    "    # For embedded_delay, look for columns with small integers (1-3)\n",
    "    if guessed_mappings['embedded_delay'] is None and numeric_cols:\n",
    "        for col in numeric_cols:\n",
    "            unique_values = sorted(df[col].unique())\n",
    "            if len(unique_values) <= 3 and set(unique_values).issubset({1, 2, 3}):\n",
    "                guessed_mappings['embedded_delay'] = col\n",
    "                break\n",
    "    \n",
    "    # For signal_length, look for columns with values like 1024, 2048, 4096\n",
    "    if guessed_mappings['signal_length'] is None and numeric_cols:\n",
    "        for col in numeric_cols:\n",
    "            unique_values = sorted(df[col].unique())\n",
    "            if len(unique_values) <= 3 and set(unique_values).issubset({1024, 2048, 4096}):\n",
    "                guessed_mappings['signal_length'] = col\n",
    "                break\n",
    "    \n",
    "    # For permutation_entropy and complexity, look for continuous values between 0 and 1\n",
    "    remaining_numeric_cols = [col for col in numeric_cols if col not in guessed_mappings.values()]\n",
    "    \n",
    "    # Permutation entropy is usually close to 1 for random signals\n",
    "    if guessed_mappings['permutation_entropy'] is None and remaining_numeric_cols:\n",
    "        for col in remaining_numeric_cols:\n",
    "            if df[col].min() >= 0 and df[col].max() <= 1:\n",
    "                guessed_mappings['permutation_entropy'] = col\n",
    "                remaining_numeric_cols.remove(col)\n",
    "                break\n",
    "    \n",
    "    # Complexity is usually smaller than permutation entropy\n",
    "    if guessed_mappings['complexity'] is None and remaining_numeric_cols:\n",
    "        for col in remaining_numeric_cols:\n",
    "            if df[col].min() >= 0 and df[col].max() <= 1:\n",
    "                guessed_mappings['complexity'] = col\n",
    "                break\n",
    "    \n",
    "    return guessed_mappings\n",
    "\n",
    "def define_custom_mappings(df):\n",
    "    \"\"\"\n",
    "    Let the user define custom column mappings.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to map\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of column mappings (original_column: mapped_column)\n",
    "    \"\"\"\n",
    "    expected_columns = [\n",
    "        'subject', 'activity', 'axis', \n",
    "        'embedded_dim', 'embedded_delay', 'signal_length', \n",
    "        'permutation_entropy', 'complexity'\n",
    "    ]\n",
    "    \n",
    "    column_mapping = {}\n",
    "    \n",
    "    print(\"\\nDefine column mappings:\")\n",
    "    for expected_col in expected_columns:\n",
    "        print(f\"\\nMap '{expected_col}' to:\")\n",
    "        for i, col in enumerate(df.columns):\n",
    "            print(f\"  {i+1}. {col}\")\n",
    "        print(\"  0. None (Skip this column)\")\n",
    "        \n",
    "        selection = input(\"Enter number: \")\n",
    "        if selection.isdigit() and int(selection) > 0 and int(selection) <= len(df.columns):\n",
    "            selected_col = df.columns[int(selection)-1]\n",
    "            column_mapping[selected_col] = expected_col\n",
    "            print(f\"Mapping {selected_col} -> {expected_col}\")\n",
    "        else:\n",
    "            print(f\"Skipping {expected_col}\")\n",
    "    \n",
    "    return column_mapping\n",
    "\n",
    "def apply_column_mapping(df, column_mapping):\n",
    "    \"\"\"\n",
    "    Apply column mapping to the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to map\n",
    "        column_mapping: Dictionary of column mappings (original_column: mapped_column)\n",
    "        \n",
    "    Returns:\n",
    "        Mapped DataFrame\n",
    "    \"\"\"\n",
    "    if not column_mapping:\n",
    "        print(\"No column mappings provided. Returning original DataFrame.\")\n",
    "        return df\n",
    "    \n",
    "    # Create a copy with mapped columns\n",
    "    mapped_df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    print(\"\\nMapped DataFrame preview:\")\n",
    "    print(mapped_df.head(3).to_string())\n",
    "    \n",
    "    # Check if all expected columns are present\n",
    "    expected_columns = [\n",
    "        'subject', 'activity', 'axis', \n",
    "        'embedded_dim', 'embedded_delay', 'signal_length', \n",
    "        'permutation_entropy', 'complexity'\n",
    "    ]\n",
    "    \n",
    "    missing_columns = [col for col in expected_columns if col not in mapped_df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"\\nWarning: Mapped DataFrame is missing columns: {missing_columns}\")\n",
    "        print(\"Some visualizations may not work properly.\")\n",
    "    \n",
    "    # Check data types and convert if necessary\n",
    "    for col in mapped_df.columns:\n",
    "        if col in ['embedded_dim', 'embedded_delay', 'signal_length']:\n",
    "            if not pd.api.types.is_numeric_dtype(mapped_df[col]):\n",
    "                try:\n",
    "                    mapped_df[col] = pd.to_numeric(mapped_df[col])\n",
    "                    print(f\"Converted '{col}' to numeric type.\")\n",
    "                except:\n",
    "                    print(f\"Warning: Could not convert '{col}' to numeric type.\")\n",
    "    \n",
    "    return mapped_df\n",
    "\n",
    "def try_direct_analysis(df):\n",
    "    \"\"\"\n",
    "    Try to analyze the DataFrame directly without mappings.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "    \"\"\"\n",
    "    print(\"\\nAttempting direct analysis...\")\n",
    "    \n",
    "    # Check if we have both 'permutation_entropy' and 'complexity' as columns\n",
    "    if 'permutation_entropy' in df.columns and 'complexity' in df.columns:\n",
    "        print(\"Found 'permutation_entropy' and 'complexity' columns directly.\")\n",
    "        generate_visualizations(df)\n",
    "        return\n",
    "    \n",
    "    # Look for columns that might contain entropy and complexity values\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if len(numeric_cols) >= 2:\n",
    "        # Take the first two numeric columns as entropy and complexity\n",
    "        pe_col = numeric_cols[0]\n",
    "        complexity_col = numeric_cols[1]\n",
    "        \n",
    "        print(f\"Using '{pe_col}' as permutation entropy and '{complexity_col}' as complexity.\")\n",
    "        \n",
    "        # Create a new DataFrame with renamed columns\n",
    "        df_renamed = df.copy()\n",
    "        df_renamed.rename(columns={pe_col: 'permutation_entropy', complexity_col: 'complexity'}, inplace=True)\n",
    "        \n",
    "        # Look for potential categorical columns for grouping\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        if categorical_cols:\n",
    "            group_col = categorical_cols[0]\n",
    "            print(f\"Using '{group_col}' for grouping (as activity).\")\n",
    "            df_renamed.rename(columns={group_col: 'activity'}, inplace=True)\n",
    "        \n",
    "        generate_visualizations(df_renamed)\n",
    "    else:\n",
    "        print(\"Could not identify appropriate columns for analysis.\")\n",
    "        print(\"Please use the CSV analyzer to understand your data structure better.\")\n",
    "\n",
    "def generate_visualizations(df, output_dir='permutation_entropy_results'):\n",
    "    \"\"\"\n",
    "    Generate visualizations based on the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with entropy and complexity data\n",
    "        output_dir: Directory to save visualizations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"\\nGenerating visualizations in '{output_dir}'...\")\n",
    "        \n",
    "        # Check required columns\n",
    "        required_cols = ['permutation_entropy', 'complexity']\n",
    "        missing_required = [col for col in required_cols if col not in df.columns]\n",
    "        \n",
    "        if missing_required:\n",
    "            print(f\"Error: Missing required columns: {missing_required}\")\n",
    "            return\n",
    "        \n",
    "        # Check grouping columns\n",
    "        has_activity = 'activity' in df.columns\n",
    "        has_embedded_dim = 'embedded_dim' in df.columns\n",
    "        has_embedded_delay = 'embedded_delay' in df.columns\n",
    "        has_signal_length = 'signal_length' in df.columns\n",
    "        has_axis = 'axis' in df.columns\n",
    "        \n",
    "        # ------ Visualization 1: Permutation Entropy Distribution ------\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(df['permutation_entropy'], kde=True)\n",
    "        plt.xlabel('Permutation Entropy')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Permutation Entropy')\n",
    "        plt.savefig(os.path.join(output_dir, 'permutation_entropy_distribution.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # ------ Visualization 2: Complexity Distribution ------\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(df['complexity'], kde=True)\n",
    "        plt.xlabel('Complexity')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Complexity')\n",
    "        plt.savefig(os.path.join(output_dir, 'complexity_distribution.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # ------ Visualization 3: Complexity vs Permutation Entropy ------\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        if has_activity:\n",
    "            sns.scatterplot(\n",
    "                x='permutation_entropy', \n",
    "                y='complexity', \n",
    "                hue='activity',\n",
    "                data=df,\n",
    "                alpha=0.7\n",
    "            )\n",
    "            plt.title('Complexity vs Permutation Entropy by Activity')\n",
    "        else:\n",
    "            sns.scatterplot(\n",
    "                x='permutation_entropy', \n",
    "                y='complexity', \n",
    "                data=df,\n",
    "                alpha=0.7\n",
    "            )\n",
    "            plt.title('Complexity vs Permutation Entropy')\n",
    "            \n",
    "        plt.xlabel('Permutation Entropy')\n",
    "        plt.ylabel('Complexity')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.savefig(os.path.join(output_dir, 'complexity_vs_entropy.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # ------ Visualization 4: Box plots if activity available ------\n",
    "        if has_activity:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.boxplot(x='activity', y='permutation_entropy', data=df)\n",
    "            plt.xlabel('Activity')\n",
    "            plt.ylabel('Permutation Entropy')\n",
    "            plt.title('Distribution of Permutation Entropy by Activity')\n",
    "            plt.grid(True, axis='y')\n",
    "            plt.savefig(os.path.join(output_dir, 'permutation_entropy_boxplot.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.boxplot(x='activity', y='complexity', data=df)\n",
    "            plt.xlabel('Activity')\n",
    "            plt.ylabel('Complexity')\n",
    "            plt.title('Distribution of Complexity by Activity')\n",
    "            plt.grid(True, axis='y')\n",
    "            plt.savefig(os.path.join(output_dir, 'complexity_boxplot.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        # ------ Visualization 5: Embedded dimension effect if available ------\n",
    "        if has_embedded_dim and has_activity:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            for activity in df['activity'].unique():\n",
    "                activity_data = df[df['activity'] == activity]\n",
    "                \n",
    "                # Average over other parameters\n",
    "                avg_data = activity_data.groupby('embedded_dim')['permutation_entropy'].mean()\n",
    "                std_data = activity_data.groupby('embedded_dim')['permutation_entropy'].std()\n",
    "                \n",
    "                plt.errorbar(\n",
    "                    avg_data.index, \n",
    "                    avg_data.values, \n",
    "                    yerr=std_data.values,\n",
    "                    marker='o',\n",
    "                    label=activity\n",
    "                )\n",
    "            \n",
    "            plt.xlabel('Embedding Dimension')\n",
    "            plt.ylabel('Average Permutation Entropy')\n",
    "            plt.title('Permutation Entropy by Activity and Embedding Dimension')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.savefig(os.path.join(output_dir, 'permutation_entropy_by_embedding_dim.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        # ------ Visualization 6: Signal length effect if available ------\n",
    "        if has_signal_length and has_activity:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            for activity in df['activity'].unique():\n",
    "                activity_data = df[df['activity'] == activity]\n",
    "                \n",
    "                # Average over other parameters\n",
    "                avg_data = activity_data.groupby('signal_length')['permutation_entropy'].mean()\n",
    "                std_data = activity_data.groupby('signal_length')['permutation_entropy'].std()\n",
    "                \n",
    "                plt.errorbar(\n",
    "                    avg_data.index, \n",
    "                    avg_data.values, \n",
    "                    yerr=std_data.values,\n",
    "                    marker='o',\n",
    "                    label=activity\n",
    "                )\n",
    "            \n",
    "            plt.xlabel('Signal Length')\n",
    "            plt.ylabel('Average Permutation Entropy')\n",
    "            plt.title('Permutation Entropy by Activity and Signal Length')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.savefig(os.path.join(output_dir, 'permutation_entropy_by_signal_length.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        # ------ Visualization 7: Parameters heatmap if available ------\n",
    "        if has_embedded_dim and has_embedded_delay:\n",
    "            dim_delay_data = df.groupby(['embedded_dim', 'embedded_delay'])['permutation_entropy'].mean().reset_index()\n",
    "            \n",
    "            # Create pivot table\n",
    "            pivot_data = dim_delay_data.pivot('embedded_dim', 'embedded_delay', 'permutation_entropy')\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(pivot_data, annot=True, cmap='viridis', fmt='.3f')\n",
    "            plt.title('Average Permutation Entropy for Different Parameter Combinations')\n",
    "            plt.ylabel('Embedded Dimension')\n",
    "            plt.xlabel('Embedded Delay')\n",
    "            \n",
    "            plt.savefig(os.path.join(output_dir, 'parameter_heatmap.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        # ------ Visualization 8: Axis comparison if available ------\n",
    "        if has_axis and has_activity:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            sns.boxplot(x='activity', y='permutation_entropy', hue='axis', data=df)\n",
    "            plt.xlabel('Activity')\n",
    "            plt.ylabel('Permutation Entropy')\n",
    "            plt.title('Permutation Entropy by Activity and Axis')\n",
    "            plt.grid(True, axis='y')\n",
    "            \n",
    "            plt.savefig(os.path.join(output_dir, 'permutation_entropy_by_axis.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        print(f\"Visualization complete! {len(os.listdir(output_dir))} files saved to '{output_dir}'\")\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        \n",
    "        # Overall statistics\n",
    "        print(\"\\nOverall Permutation Entropy:\")\n",
    "        print(df['permutation_entropy'].describe())\n",
    "        \n",
    "        print(\"\\nOverall Complexity:\")\n",
    "        print(df['complexity'].describe())\n",
    "        \n",
    "        # Statistics by activity if available\n",
    "        if has_activity:\n",
    "            print(\"\\nPermutation Entropy by Activity:\")\n",
    "            print(df.groupby('activity')['permutation_entropy'].describe())\n",
    "            \n",
    "            print(\"\\nComplexity by Activity:\")\n",
    "            print(df.groupby('activity')['complexity'].describe())\n",
    "            \n",
    "            # Look for statistically significant differences\n",
    "            if len(df['activity'].unique()) >= 2:\n",
    "                print(\"\\nActivity Comparison (mean values):\")\n",
    "                activity_means = df.groupby('activity')[['permutation_entropy', 'complexity']].mean()\n",
    "                print(activity_means)\n",
    "                \n",
    "                # Calculate differences between activities\n",
    "                activities = df['activity'].unique()\n",
    "                print(\"\\nPairwise Differences in Permutation Entropy:\")\n",
    "                for i, act1 in enumerate(activities):\n",
    "                    for act2 in activities[i+1:]:\n",
    "                        diff = activity_means.loc[act1, 'permutation_entropy'] - activity_means.loc[act2, 'permutation_entropy']\n",
    "                        print(f\"  {act1} vs {act2}: {diff:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating visualizations: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Flexible Permutation Entropy Analysis\")\n",
    "    print(\"------------------------------------\")\n",
    "    file_path = input(\"Enter the path to the CSV file: \")\n",
    "    analyze_csv_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ca9d1a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1207426452.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    cat output.csv\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# To view the analysis results\n",
    "cat output.csv\n",
    "\n",
    "# To view the plot (this will open it in your default image viewer)\n",
    "open output_plot.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbaec69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (numpy_1.21_env)",
   "language": "python",
   "name": "numpy_1.21_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
