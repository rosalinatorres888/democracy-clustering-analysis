{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4674b65",
   "metadata": {},
   "source": [
    "# Network & Word Frequency Analysis: Technical Implementation and Findings\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, I conducted a comprehensive keyword co-occurrence network analysis to explore relationships between academic concepts in data mining literature. Using both Python-based network analysis and a custom React-based interactive visualization, I uncovered meaningful patterns that provide insights into the structure of knowledge in this domain.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "I began with a thorough preprocessing of the keyword data to ensure accurate analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8019b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c12f994",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/rosalinatorres/downloads/co_occurrence_matrix.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the co-occurrence matrix from the CSV file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/rosalinatorres/downloads/co_occurrence_matrix.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m co_occurrence_matrix \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Define a cleaning function for standardizing keywords\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_keyword\u001b[39m(keyword: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/rosalinatorres/downloads/co_occurrence_matrix.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the co-occurrence matrix from the CSV file\n",
    "file_path = \"/Users/rosalinatorres/downloads/co_occurrence_matrix.csv\"\n",
    "co_occurrence_matrix = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "# Define a cleaning function for standardizing keywords\n",
    "def clean_keyword(keyword: str) -> str:\n",
    "    \"\"\"Cleans a keyword string by removing double hyphens and standardizing whitespace.\"\"\"\n",
    "    cleaned_keyword = \" \".join(keyword.split(\"--\"))  # Replace \"--\" with space\n",
    "    cleaned_keyword = \" \".join(cleaned_keyword.split())  # Remove extra spaces\n",
    "    return cleaned_keyword.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# Apply the cleaning function to row and column names\n",
    "co_occurrence_matrix.columns = co_occurrence_matrix.columns.map(clean_keyword)\n",
    "co_occurrence_matrix.index = co_occurrence_matrix.index.map(clean_keyword)\n",
    "\n",
    "# Ensure that column and index names are lowercase for consistency\n",
    "co_occurrence_matrix.columns = co_occurrence_matrix.columns.str.lower()\n",
    "co_occurrence_matrix.index = co_occurrence_matrix.index.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d34933",
   "metadata": {},
   "source": [
    "### Network Construction\n",
    "\n",
    "I constructed a weighted network graph where nodes represent keywords and edges represent their co-occurrence frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty undirected graph to store the weighted network\n",
    "G_weighted = nx.Graph()\n",
    "\n",
    "# Iterate over the co-occurrence matrix and add edges to the graph\n",
    "for word1 in co_occurrence_matrix.index:\n",
    "    for word2 in co_occurrence_matrix.columns:\n",
    "        weight = co_occurrence_matrix.at[word1, word2]\n",
    "        # Skip the pair if the weight is zero or NaN\n",
    "        if pd.notna(weight) and weight > 0:\n",
    "            G_weighted.add_edge(word1, word2, weight=weight)\n",
    "\n",
    "# Display basic information about the graph\n",
    "print(f\"Number of Nodes: {G_weighted.number_of_nodes()}\")\n",
    "print(f\"Number of Edges: {G_weighted.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e154e",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### 1. Network Structure Analysis\n",
    "\n",
    "My analysis of the network revealed a complex structure with distinct patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4524dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic network metrics\n",
    "density = nx.density(G_weighted)\n",
    "avg_clustering = nx.average_clustering(G_weighted)\n",
    "avg_path_length = nx.average_shortest_path_length(G_weighted)\n",
    "diameter = nx.diameter(G_weighted)\n",
    "\n",
    "print(f\"Network Density: {density:.4f}\")\n",
    "print(f\"Average Clustering Coefficient: {avg_clustering:.4f}\")\n",
    "print(f\"Average Path Length: {avg_path_length:.4f}\")\n",
    "print(f\"Network Diameter: {diameter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caaaa8f",
   "metadata": {},
   "source": [
    "**Key Network Properties:**\n",
    "- The network showed a relatively low density, indicating a selective pattern of keyword co-occurrences\n",
    "- The high clustering coefficient suggests well-formed thematic communities\n",
    "- The short average path length demonstrates the \"small world\" property typical of knowledge networks\n",
    "\n",
    "### 2. Centrality Analysis\n",
    "\n",
    "I identified the most influential keywords through various centrality measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centrality measures\n",
    "degree_centrality = nx.degree_centrality(G_weighted)\n",
    "betweenness_centrality = nx.betweenness_centrality(G_weighted)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G_weighted)\n",
    "\n",
    "# Display top keywords by degree centrality\n",
    "top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\nTop Keywords by Degree Centrality:\")\n",
    "for keyword, centrality in top_degree:\n",
    "    print(f\"{keyword}: {centrality:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9474bb4",
   "metadata": {},
   "source": [
    "**Centrality Findings:**\n",
    "- The most central keywords (by degree) were \"machine learning,\" \"data mining,\" and \"social networks\"\n",
    "- High betweenness centrality keywords like \"analytics\" and \"security\" function as bridging concepts\n",
    "- Eigenvector centrality revealed the broader influence of concepts like \"big data\" and \"artificial intelligence\"\n",
    "\n",
    "### 3. Community Detection\n",
    "\n",
    "I used community detection algorithms to identify thematic clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b89e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import community detection algorithm\n",
    "from community import community_louvain\n",
    "\n",
    "# Apply Louvain method for community detection\n",
    "partition = community_louvain.best_partition(G_weighted)\n",
    "\n",
    "# Organize communities\n",
    "communities = {}\n",
    "for node, community_id in partition.items():\n",
    "    if community_id not in communities:\n",
    "        communities[community_id] = []\n",
    "    communities[community_id].append(node)\n",
    "\n",
    "# Display community statistics\n",
    "print(f\"\\nNumber of Communities: {len(communities)}\")\n",
    "for community_id, nodes in communities.items():\n",
    "    print(f\"Community {community_id}: {len(nodes)} keywords\")\n",
    "    # Print top 5 most central nodes in each community\n",
    "    community_centrality = {node: degree_centrality[node] for node in nodes}\n",
    "    top_nodes = sorted(community_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(f\"  Top keywords: {', '.join(node for node, _ in top_nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5480cc",
   "metadata": {},
   "source": [
    "**Community Analysis:**\n",
    "- I identified three major thematic clusters in the network:\n",
    "  1. **Data Science Cluster**: Focused on technical methods and algorithms\n",
    "  2. **Business Applications Cluster**: Emphasizing practical business uses\n",
    "  3. **Information Management Cluster**: Dealing with system and resource aspects\n",
    "\n",
    "- Each community has distinctive bridge nodes that connect it to other communities\n",
    "\n",
    "### 4. Static Network Visualization\n",
    "\n",
    "I created a static visualization to provide an overview of the network structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d899b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spring layout for better visualization\n",
    "pos = nx.spring_layout(G_weighted, k=0.1, iterations=20, seed=42)\n",
    "\n",
    "# Set up colors based on communities\n",
    "community_colors = [partition.get(node, 0) for node in G_weighted.nodes()]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(20, 20))\n",
    "nx.draw_networkx(\n",
    "    G_weighted, \n",
    "    pos=pos,\n",
    "    with_labels=True,\n",
    "    node_color=community_colors,\n",
    "    node_size=[v * 5000 + 100 for v in degree_centrality.values()],\n",
    "    width=[d['weight'] * 0.1 for u, v, d in G_weighted.edges(data=True)],\n",
    "    edge_color='lightgray',\n",
    "    cmap=plt.cm.viridis,\n",
    "    font_size=8\n",
    ")\n",
    "\n",
    "plt.title(\"Keyword Co-occurrence Network with Communities\", fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"network_visualization.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7465eeee",
   "metadata": {},
   "source": [
    "## Interactive Visualization\n",
    "\n",
    "To enable dynamic exploration of the network, I developed an interactive visualization using React and HTML Canvas:\n",
    "\n",
    "```jsx\n",
    "// NetworkVisualization.jsx - Core physics simulation\n",
    "\n",
    "const applyForces = () => {\n",
    "  const nodes = graphData.nodes;\n",
    "  const links = graphData.links;\n",
    "  \n",
    "  // Constants - adjusted for better visibility\n",
    "  const centerX = canvasSize.width / 2;\n",
    "  const centerY = canvasSize.height / 2;\n",
    "  const centerForce = 0.0003;\n",
    "  const repulsionForce = 700;\n",
    "  const linkStrength = 0.02;\n",
    "  const damping = 0.85;\n",
    "  \n",
    "  // Calculate forces\n",
    "  nodes.forEach(node => {\n",
    "    // Initialize forces\n",
    "    node.fx = 0;\n",
    "    node.fy = 0;\n",
    "    \n",
    "    // Center attraction force\n",
    "    node.fx += (centerX - node.x) * centerForce;\n",
    "    node.fy += (centerY - node.y) * centerForce;\n",
    "    \n",
    "    // Node repulsion (inverse square law)\n",
    "    nodes.forEach(otherNode => {\n",
    "      if (node !== otherNode) {\n",
    "        const dx = node.x - otherNode.x;\n",
    "        const dy = node.y - otherNode.y;\n",
    "        const distance = Math.sqrt(dx * dx + dy * dy);\n",
    "        const forceMagnitude = repulsionForce / Math.max(10, distance * distance);\n",
    "        \n",
    "        if (distance > 0) {\n",
    "          node.fx += (dx / distance) * forceMagnitude;\n",
    "          node.fy += (dy / distance) * forceMagnitude;\n",
    "        }\n",
    "      }\n",
    "    });\n",
    "  });\n",
    "  \n",
    "  // Additional force simulation code...\n",
    "};\n",
    "```\n",
    "\n",
    "The interactive visualization includes several key features:\n",
    "1. **Force-directed layout** that positions related keywords close together\n",
    "2. **Color-coding of nodes** based on frequency or community membership\n",
    "3. **Interactive node selection** for detailed information display\n",
    "4. **Dynamic hover effects** for easier network exploration\n",
    "5. **Network statistics panel** providing quantitative context\n",
    "\n",
    "## Discussion & Interpretation\n",
    "\n",
    "Through this network analysis, I've uncovered several important insights:\n",
    "\n",
    "### 1. Knowledge Structure\n",
    "\n",
    "The network structure reveals how knowledge in this domain is organized:\n",
    "- The core-periphery pattern shows established foundational concepts at the center\n",
    "- The clear community structure demonstrates specialization within the broader field\n",
    "- The bridging nodes highlight concepts that facilitate cross-disciplinary knowledge transfer\n",
    "\n",
    "### 2. Research Opportunities\n",
    "\n",
    "My analysis points to several promising research directions:\n",
    "- The sparse connections between certain communities suggest opportunities for integration\n",
    "- Peripheral nodes with significant connections may represent emerging research areas\n",
    "- High betweenness centrality keywords indicate potential focal points for interdisciplinary work\n",
    "\n",
    "### 3. Methodological Contributions\n",
    "\n",
    "This project demonstrates the value of combining:\n",
    "- Rigorous network science methods for quantitative analysis\n",
    "- Interactive visualization techniques for intuitive exploration\n",
    "- Cross-platform implementation (Python for analysis, JavaScript for visualization)\n",
    "\n",
    "## Limitations & Future Work\n",
    "\n",
    "While this analysis provides valuable insights, I acknowledge several limitations:\n",
    "- The static nature of the dataset doesn't capture temporal evolution\n",
    "- The analysis focuses on co-occurrence rather than semantic relationships\n",
    "- The current implementation has performance limitations with very large networks\n",
    "\n",
    "In future work, I plan to address these limitations by:\n",
    "- Incorporating temporal data to track concept evolution\n",
    "- Exploring semantic analysis techniques to capture deeper relationships\n",
    "- Implementing performance optimizations for larger networks\n",
    "- Adding additional interactive features like filtering and search\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This network and word frequency analysis provides a comprehensive view of the conceptual landscape in data mining research. Through the combination of rigorous network analysis and interactive visualization, I've demonstrated how these methods can reveal patterns and relationships that contribute to our understanding of knowledge organization in this domain.\n",
    "\n",
    "The interactive visualization component not only makes these findings more accessible but also showcases the potential of web-based tools for exploring complex network data. This integrated approach offers a valuable perspective for researchers, educators, and practitioners seeking to navigate and understand this complex intellectual landscape.\n",
    "\n",
    "---\n",
    "\n",
    "This project was developed by Rosalina Torres as part of advanced data mining and visualization coursework."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (numpy_1.21_env)",
   "language": "python",
   "name": "numpy_1.21_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
