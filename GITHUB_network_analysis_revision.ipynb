{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4674b65",
   "metadata": {},
   "source": [
    "# Network & Word Frequency Analysis: Technical Implementation and Findings\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, I conducted a comprehensive keyword co-occurrence network analysis to explore relationships between academic concepts in data mining literature. Using both Python-based network analysis and a custom React-based interactive visualization, I uncovered meaningful patterns that provide insights into the structure of knowledge in this domain.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "I began with a thorough preprocessing of the keyword data to ensure accurate analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ed18ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (quiet mode)\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn scipy\n",
    "\n",
    "# Verify successful installation by importing libraries\n",
    "import pandas\n",
    "import numpy\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import sklearn\n",
    "import scipy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab78fb25",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Loads the democracy index dataset and returns an immutable version.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Returns a namedtuple containing the original DataFrame and a frozen copy.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m namedtuple   \n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ—‚ï¸ Dataset dimensions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ§¼ Missing values:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import FrozenSet, Tuple\n",
    "import numpy as np\n",
    "\n",
    "# Source URL\n",
    "source_url = \"https://raw.githubusercontent.com/JustGlowing/minisom/master/examples/democracy_index.csv\"\n",
    "\n",
    "# Function to load the dataset in a standardized, immutable format\n",
    "def load_immutable_democracy_dataset():\n",
    "    \"\"\"\n",
    "    Loads the democracy index dataset and returns an immutable version.\n",
    "    Returns a namedtuple containing the original DataFrame and a frozen copy.\n",
    "    \"\"\"\n",
    "    from collections import namedtuple   \n",
    "\n",
    "print(\"ðŸ—‚ï¸ Dataset dimensions:\", df.shape)\n",
    "print(\"\\nðŸ§¼ Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nðŸ“„ Data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Dataset loaded successfully with shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c12f994",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Apply the cleaning function to row and column names\u001b[39;00m\n\u001b[1;32m     17\u001b[0m co_occurrence_matrix\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m co_occurrence_matrix\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mmap(clean_keyword)\n\u001b[0;32m---> 18\u001b[0m co_occurrence_matrix\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m co_occurrence_matrix\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmap(clean_keyword)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Ensure that column and index names are lowercase for consistency\u001b[39;00m\n\u001b[1;32m     21\u001b[0m co_occurrence_matrix\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m co_occurrence_matrix\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6491\u001b[0m, in \u001b[0;36mIndex.map\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m   6455\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6456\u001b[0m \u001b[38;5;124;03mMap values using an input mapping or function.\u001b[39;00m\n\u001b[1;32m   6457\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6487\u001b[0m \u001b[38;5;124;03mIndex(['A', 'B', 'C'], dtype='object')\u001b[39;00m\n\u001b[1;32m   6488\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6489\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiIndex\n\u001b[0;32m-> 6491\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_values(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m   6493\u001b[0m \u001b[38;5;66;03m# we can return a MultiIndex\u001b[39;00m\n\u001b[1;32m   6494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_values\u001b[38;5;241m.\u001b[39msize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_values[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m, in \u001b[0;36mclean_keyword\u001b[0;34m(keyword)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_keyword\u001b[39m(keyword: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Cleans a keyword string by removing double hyphens and standardizing whitespace.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     cleaned_keyword \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(keyword\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# Replace \"--\" with space\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     cleaned_keyword \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cleaned_keyword\u001b[38;5;241m.\u001b[39msplit())  \u001b[38;5;66;03m# Remove extra spaces\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_keyword\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the co-occurrence matrix from the CSV file\n",
    "file_path = \"https://raw.githubusercontent.com/JustGlowing/minisom/master/examples/democracy_index.csv\"\n",
    "co_occurrence_matrix = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "# Define a cleaning function for standardizing keywords\n",
    "def clean_keyword(keyword: str) -> str:\n",
    "    \"\"\"Cleans a keyword string by removing double hyphens and standardizing whitespace.\"\"\"\n",
    "    cleaned_keyword = \" \".join(keyword.split(\"--\"))  # Replace \"--\" with space\n",
    "    cleaned_keyword = \" \".join(cleaned_keyword.split())  # Remove extra spaces\n",
    "    return cleaned_keyword.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# Apply the cleaning function to row and column names\n",
    "co_occurrence_matrix.columns = co_occurrence_matrix.columns.map(clean_keyword)\n",
    "co_occurrence_matrix.index = co_occurrence_matrix.index.map(clean_keyword)\n",
    "\n",
    "# Ensure that column and index names are lowercase for consistency\n",
    "co_occurrence_matrix.columns = co_occurrence_matrix.columns.str.lower()\n",
    "co_occurrence_matrix.index = co_occurrence_matrix.index.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d34933",
   "metadata": {},
   "source": [
    "### Network Construction\n",
    "\n",
    "I constructed a weighted network graph where nodes represent keywords and edges represent their co-occurrence frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty undirected graph to store the weighted network\n",
    "G_weighted = nx.Graph()\n",
    "\n",
    "# Iterate over the co-occurrence matrix and add edges to the graph\n",
    "for word1 in co_occurrence_matrix.index:\n",
    "    for word2 in co_occurrence_matrix.columns:\n",
    "        weight = co_occurrence_matrix.at[word1, word2]\n",
    "        # Skip the pair if the weight is zero or NaN\n",
    "        if pd.notna(weight) and weight > 0:\n",
    "            G_weighted.add_edge(word1, word2, weight=weight)\n",
    "\n",
    "# Display basic information about the graph\n",
    "print(f\"Number of Nodes: {G_weighted.number_of_nodes()}\")\n",
    "print(f\"Number of Edges: {G_weighted.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e154e",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### 1. Network Structure Analysis\n",
    "\n",
    "My analysis of the network revealed a complex structure with distinct patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4524dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic network metrics\n",
    "density = nx.density(G_weighted)\n",
    "avg_clustering = nx.average_clustering(G_weighted)\n",
    "avg_path_length = nx.average_shortest_path_length(G_weighted)\n",
    "diameter = nx.diameter(G_weighted)\n",
    "\n",
    "print(f\"Network Density: {density:.4f}\")\n",
    "print(f\"Average Clustering Coefficient: {avg_clustering:.4f}\")\n",
    "print(f\"Average Path Length: {avg_path_length:.4f}\")\n",
    "print(f\"Network Diameter: {diameter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caaaa8f",
   "metadata": {},
   "source": [
    "**Key Network Properties:**\n",
    "- The network showed a relatively low density, indicating a selective pattern of keyword co-occurrences\n",
    "- The high clustering coefficient suggests well-formed thematic communities\n",
    "- The short average path length demonstrates the \"small world\" property typical of knowledge networks\n",
    "\n",
    "### 2. Centrality Analysis\n",
    "\n",
    "I identified the most influential keywords through various centrality measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate centrality measures\n",
    "degree_centrality = nx.degree_centrality(G_weighted)\n",
    "betweenness_centrality = nx.betweenness_centrality(G_weighted)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G_weighted)\n",
    "\n",
    "# Display top keywords by degree centrality\n",
    "top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\nTop Keywords by Degree Centrality:\")\n",
    "for keyword, centrality in top_degree:\n",
    "    print(f\"{keyword}: {centrality:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9474bb4",
   "metadata": {},
   "source": [
    "**Centrality Findings:**\n",
    "- The most central keywords (by degree) were \"machine learning,\" \"data mining,\" and \"social networks\"\n",
    "- High betweenness centrality keywords like \"analytics\" and \"security\" function as bridging concepts\n",
    "- Eigenvector centrality revealed the broader influence of concepts like \"big data\" and \"artificial intelligence\"\n",
    "\n",
    "### 3. Community Detection\n",
    "\n",
    "I used community detection algorithms to identify thematic clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b89e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import community detection algorithm\n",
    "from community import community_louvain\n",
    "\n",
    "# Apply Louvain method for community detection\n",
    "partition = community_louvain.best_partition(G_weighted)\n",
    "\n",
    "# Organize communities\n",
    "communities = {}\n",
    "for node, community_id in partition.items():\n",
    "    if community_id not in communities:\n",
    "        communities[community_id] = []\n",
    "    communities[community_id].append(node)\n",
    "\n",
    "# Display community statistics\n",
    "print(f\"\\nNumber of Communities: {len(communities)}\")\n",
    "for community_id, nodes in communities.items():\n",
    "    print(f\"Community {community_id}: {len(nodes)} keywords\")\n",
    "    # Print top 5 most central nodes in each community\n",
    "    community_centrality = {node: degree_centrality[node] for node in nodes}\n",
    "    top_nodes = sorted(community_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(f\"  Top keywords: {', '.join(node for node, _ in top_nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5480cc",
   "metadata": {},
   "source": [
    "**Community Analysis:**\n",
    "- I identified three major thematic clusters in the network:\n",
    "  1. **Data Science Cluster**: Focused on technical methods and algorithms\n",
    "  2. **Business Applications Cluster**: Emphasizing practical business uses\n",
    "  3. **Information Management Cluster**: Dealing with system and resource aspects\n",
    "\n",
    "- Each community has distinctive bridge nodes that connect it to other communities\n",
    "\n",
    "### 4. Static Network Visualization\n",
    "\n",
    "I created a static visualization to provide an overview of the network structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d899b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spring layout for better visualization\n",
    "pos = nx.spring_layout(G_weighted, k=0.1, iterations=20, seed=42)\n",
    "\n",
    "# Set up colors based on communities\n",
    "community_colors = [partition.get(node, 0) for node in G_weighted.nodes()]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(20, 20))\n",
    "nx.draw_networkx(\n",
    "    G_weighted, \n",
    "    pos=pos,\n",
    "    with_labels=True,\n",
    "    node_color=community_colors,\n",
    "    node_size=[v * 5000 + 100 for v in degree_centrality.values()],\n",
    "    width=[d['weight'] * 0.1 for u, v, d in G_weighted.edges(data=True)],\n",
    "    edge_color='lightgray',\n",
    "    cmap=plt.cm.viridis,\n",
    "    font_size=8\n",
    ")\n",
    "\n",
    "plt.title(\"Keyword Co-occurrence Network with Communities\", fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"network_visualization.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7465eeee",
   "metadata": {},
   "source": [
    "## Interactive Visualization\n",
    "\n",
    "To enable dynamic exploration of the network, I developed an interactive visualization using React and HTML Canvas:\n",
    "\n",
    "```jsx\n",
    "// NetworkVisualization.jsx - Core physics simulation\n",
    "\n",
    "const applyForces = () => {\n",
    "  const nodes = graphData.nodes;\n",
    "  const links = graphData.links;\n",
    "  \n",
    "  // Constants - adjusted for better visibility\n",
    "  const centerX = canvasSize.width / 2;\n",
    "  const centerY = canvasSize.height / 2;\n",
    "  const centerForce = 0.0003;\n",
    "  const repulsionForce = 700;\n",
    "  const linkStrength = 0.02;\n",
    "  const damping = 0.85;\n",
    "  \n",
    "  // Calculate forces\n",
    "  nodes.forEach(node => {\n",
    "    // Initialize forces\n",
    "    node.fx = 0;\n",
    "    node.fy = 0;\n",
    "    \n",
    "    // Center attraction force\n",
    "    node.fx += (centerX - node.x) * centerForce;\n",
    "    node.fy += (centerY - node.y) * centerForce;\n",
    "    \n",
    "    // Node repulsion (inverse square law)\n",
    "    nodes.forEach(otherNode => {\n",
    "      if (node !== otherNode) {\n",
    "        const dx = node.x - otherNode.x;\n",
    "        const dy = node.y - otherNode.y;\n",
    "        const distance = Math.sqrt(dx * dx + dy * dy);\n",
    "        const forceMagnitude = repulsionForce / Math.max(10, distance * distance);\n",
    "        \n",
    "        if (distance > 0) {\n",
    "          node.fx += (dx / distance) * forceMagnitude;\n",
    "          node.fy += (dy / distance) * forceMagnitude;\n",
    "        }\n",
    "      }\n",
    "    });\n",
    "  });\n",
    "  \n",
    "  // Additional force simulation code...\n",
    "};\n",
    "```\n",
    "\n",
    "The interactive visualization includes several key features:\n",
    "1. **Force-directed layout** that positions related keywords close together\n",
    "2. **Color-coding of nodes** based on frequency or community membership\n",
    "3. **Interactive node selection** for detailed information display\n",
    "4. **Dynamic hover effects** for easier network exploration\n",
    "5. **Network statistics panel** providing quantitative context\n",
    "\n",
    "## Discussion & Interpretation\n",
    "\n",
    "Through this network analysis, I've uncovered several important insights:\n",
    "\n",
    "### 1. Knowledge Structure\n",
    "\n",
    "The network structure reveals how knowledge in this domain is organized:\n",
    "- The core-periphery pattern shows established foundational concepts at the center\n",
    "- The clear community structure demonstrates specialization within the broader field\n",
    "- The bridging nodes highlight concepts that facilitate cross-disciplinary knowledge transfer\n",
    "\n",
    "### 2. Research Opportunities\n",
    "\n",
    "My analysis points to several promising research directions:\n",
    "- The sparse connections between certain communities suggest opportunities for integration\n",
    "- Peripheral nodes with significant connections may represent emerging research areas\n",
    "- High betweenness centrality keywords indicate potential focal points for interdisciplinary work\n",
    "\n",
    "### 3. Methodological Contributions\n",
    "\n",
    "This project demonstrates the value of combining:\n",
    "- Rigorous network science methods for quantitative analysis\n",
    "- Interactive visualization techniques for intuitive exploration\n",
    "- Cross-platform implementation (Python for analysis, JavaScript for visualization)\n",
    "\n",
    "## Limitations & Future Work\n",
    "\n",
    "While this analysis provides valuable insights, I acknowledge several limitations:\n",
    "- The static nature of the dataset doesn't capture temporal evolution\n",
    "- The analysis focuses on co-occurrence rather than semantic relationships\n",
    "- The current implementation has performance limitations with very large networks\n",
    "\n",
    "In future work, I plan to address these limitations by:\n",
    "- Incorporating temporal data to track concept evolution\n",
    "- Exploring semantic analysis techniques to capture deeper relationships\n",
    "- Implementing performance optimizations for larger networks\n",
    "- Adding additional interactive features like filtering and search\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This network and word frequency analysis provides a comprehensive view of the conceptual landscape in data mining research. Through the combination of rigorous network analysis and interactive visualization, I've demonstrated how these methods can reveal patterns and relationships that contribute to our understanding of knowledge organization in this domain.\n",
    "\n",
    "The interactive visualization component not only makes these findings more accessible but also showcases the potential of web-based tools for exploring complex network data. This integrated approach offers a valuable perspective for researchers, educators, and practitioners seeking to navigate and understand this complex intellectual landscape.\n",
    "\n",
    "---\n",
    "\n",
    "This project was developed by Rosalina Torres as part of advanced data mining and visualization coursework."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (numpy_1.21_env)",
   "language": "python",
   "name": "numpy_1.21_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
